
\subsection{Decision Trees}
\label{sec:decision-tree}

Building decision trees is a supervised learning process used to approximate a target function as a sequence of disjunctive conditions designed to measure the effectiveness of a set of attributes to classify data and predict outcomes. The theory behind decision trees is well established \citep[e.g.,][]{Quinlan_1986_ML, Quinlan_1993_Book, Mitchell_1997_Book}.
% \myrevision{A decision tree can be used to classify a case by starting at the root of the tree and moving through it until the leaf is encountered. Each tree is a structure including a root, decision nodes and leaf nodes. Decision nodes specifies some test to carried out on attributes and leaf nodes indicate the final class \citep{Quinlan_1993_Book}.}
Decision trees are, however, non-unique. They depend on the dataset, the user parameters, and the algorithm employed to build them \citep{Murthy_1998_DMKD}. Therefore, before describing our choice of the decision tree algorithm and parameters, we revise three aspects about the dataset: data classes, attributes equivalence, and dataset balance.

First, by data classes we refer to whether the classes of data are properly distinguishable or not, and to whether the data attributes contribute to those classes or not. In our case, data classes are handled by means of the clustering process. As we described in the previous section, at the end of the clustering process, all the data samples in our dataset are labeled \change{into one out the four validation categories. In decision trees, however, we refer to these categories as \textit{classes}. We are therefore interested in working with a labeled dataset or a properly \textit{classified} dataset using the validation classes P, F, G and E, which is something that has been guaranteed by the prior clustering process.}

% outcome classes (or clusters) defined in terms of the validation categories (poor, fair, good, excellent).

Second, by attributes equivalence, we refer to whether the data attributes are comparable with each other on equivalent terms or not. There are cases, either because of the method or because of the data, that the data attributes need to be standardized. In artificial neural networks, for instance, the data needs to be normalized on a $[-1,1]$ or $[0,1]$ scale. Normalization is also necessary when data attributes come in different unit scales or value ranges \citep[e.g.,][]{Wu_2010_JH}. In our case, \change{the concept of \textit{attributes} in the context of the decision tree analysis is the same as the concept of \textit{features} in clustering or GOF metrics in validation. Since all our metrics are dimensionless and defined within the same numerical scale (varying from 0 to 10), normalization is not necessary and the process of building a decision tree} is not susceptible to the attributes scales.

% In our case, this is not necessary because (a) all the data attributes---defined by the 11 GOF metrics---are on the same dimensionless numerical scale (varying from 0 to 10), and (b) because the algorithm we use is not susceptible to the attributes scale.

Last, there is the issue of balance. A balanced dataset is one that has about the same number of samples per class. Algorithms used for building decision trees tend to perform better with balanced datasets \citep[e.g.,][]{Branco_2016_ACMCS, Weiss_2003_JAIR}. Imbalanced datasets, on the other hand, are those with a significant disparity in the number of samples in each class. Imbalanced datasets can be improved by a resampling process. There are two basic resampling approaches: undersampling and oversampling \citep{Branco_2016_ACMCS}. Undersampling discards data from the subsets with larger number of samples. Oversampling replicates data until reaching appropriate number of samples. Both processes are done by randomly picking data to either be discarded or replicated. In our case, as we will see later, we apply oversampling to one of our dataset classes.

\change{In general, oversampling increases the possibility of overfitting, which occurs when the training data leads the algorithm to produce a decision tree that predicts outcomes too close to or exactly the same as the training data. This is not desirable because such a tree would lead to inaccurate predictions for other unseen data. Overfitting can be avoided 
% by using conservative oversampling rates \citep{Weiss_2003_JAIR} and/or
by applying heavy pruning methods. (Pruning, as the word implies, is the process of cutting branches off a tree. Heavy pruning methods limit the complexity of the tree by constraining the number of branches and/or depth of a tree.)}

% This process has consequences, mainly that it increases the likelihood of overfitting. \myrevision{Overfitting happens when the prediction algorithm predicts the final results too closely or exactly to the training data, however, it provides non-accurate results for unseen or test data.}

Upon completing the clustering and resampling processes, the next step is to subdivide the dataset in two, a training dataset (with 70 percent of the samples, picked randomly), and a testing dataset (with the remaining 30 percent). We first use the training dataset to build a large suite of potential decision trees, and then use the testing dataset to evaluate and pick the best possible decision tree(s). There are several algorithms for building decision trees \citep[e.g., ID3, C4.5, C5.0, CART; see][]{Quinlan_1986_ML, Quinlan_1993_Book, Quinlan_1996_JAIR, Breiman_1984_Book}. \change{Here, we use the C5.0 algorithm as implemented by \citet{Kuhn_2017_Manual}. This algorithm is the latest update to the original ID3 and subsequent C4.5 algorithms \citep{Quinlan_1993_Book, Quinlan_1996_JAIR}. C5.0 is superior to its predecessors because it reduces the limitations for handling numerical attributes, missing data, and it has additional features such as the development of boosted models. Discussing the differences between these algorithms is out of the scope of this study.}

% \myrevision{C5.0 is considered as the developed version of ID3 and C4.5, where it does not have any limitations in handling missing data, numerical attributes, and developing boosted models. Internal algorithm of CART is different than other methods, however, discussing technical details of these algorithms are beyond the scope of this study.}

% Here we use the C5.0 implementation \citep{Kuhn_2017_Manual}, which is an extension of the C4.5 algorithm developed by \citet{Quinlan_1993_Book, Quinlan_1996_JAIR}.

In \change{general}, given a dataset, the process of building the tree consists of recursively identifying the attributes in the training data that are most likely to predict an outcome. The process is recursive because new branches and decision nodes are created based on the remaining data at every branch and level, until the tree reaches a certain depth or when other conditions are met. The effectiveness of an attribute $A$ in classifying any subset $S$ from the training data is measured through the information gain $G$ as
%
\begin{equation}
	\label{eq:gain}
	G(S,A) = E(S) - \sum_{v \, \in \, A[a]} \frac{ \left| S_v \right| }{ \left| S \right| } E \left( S_v \right)
	\, ,
\end{equation}
%
where $A[a]$ is the \change{discrete} set of all possible values of attribute $A$, $S_v$ is the subset of $S$ for which attribute $A$ has value $v$, and $E(\cdot)$ is the entropy function given by
%
\begin{equation}
	E(S) = \sum_{i=1}^{c} \Big( -p_i \log_2 \left(p_i\right) \Big)
	\, ,
\end{equation}
%
where $p_i$ is the proportion of $S$ belonging to class $i$, and $c$ are the different values that a given target attribute can take.

Entropy measures the homogeneity of a set of data. The higher the entropy, the more even the distribution of the data across classes. Conversely, an extremely low value of entropy would mean most of the data falls within a particular class. With this in mind, the information gain $G$ of an attribute $A$ for the dataset $S$, or $G(S,A)$ in equation (\ref{eq:gain}), is the expected reduction in entropy caused by partitioning the dataset according to \change{the attribute $A$ \citep{Mitchell_1997_Book}, which we consider to be discrete. For the selection of a threshold for a continuous attribute $A(a)$, please refer to \citet{Quinlan_1996_JAIR}.}

% attribute $A$ \citep{Mitchell_1997_Book}. For selection of a threshold for a \myrevision{numerical} continuous attribute refer to \citet{Quinlan_1996_JAIR}.

\change{In the particular case of the C5.0 algorithm, in addition to the general steps just described, the result of the process of building a decision tree also depends on other logical and numerical parameters. We set the options to: (a) perform feature selection or winnowing, (b) evaluate possible advanced splits of data, (c) use a confidence factor, and (d) use a threshold for the number of samples that go in the tree leaves. The feature selection option is used by the algorithm to choose the most important attribute over others; the option of evaluating advanced splits prevents the use of a hard threshold during the classification by considering different probabilities in assigning data to different classes; the confidence factor (CF) is used to control the level of pruning; and the threshold for the number of samples in a leaf is set to a minimum number $S_{\min}$ (also known as \textit{minCases}) to limit the level of complexity of the tree. Additional details about these options are available in \citet{Quinlan_1993_Book} and \citet{Kuhn_2017_Manual}. For this study, in order to obtain a variety of tree options, we build trees for varying values of CF and $S_{\min}$.}

% SUGGESTED BY NAEEM:
% In the particular case of the C5.0 algorithm the result of this process depends on several parameters, logical and numerical. We set the options to do feature selection \myrevision{or winnowing. Therefore, the algorithm choose to use the most important attributes and ignore others. We also activate the evaluation of possible advanced splits of data. Sometimes data is very close to the splitting threshold where assigning a hard threshold may incorrectly change the classification, whereas, that small difference could happen because of numerical errors or error in measurement. Therefore, advanced split can define a soft threshold to consider different probabilities in assigning data to different classes. A decision tree can grow so deep to finally classify all training data correctly. However, the tree will be very complex and will suffer from overfitting. In other words it will have very poor functionality for test data. Therefore, decision tree algorithms (here C5.0) first grow the tree up to the most complex tree then based on user defined factors start to prune the grown tree. The pruning process helps to reduce the complexity of the tree and also increase it's generalizability. Consequently the pruned tree's leaf nodes will have training data which belongs to more than one class which the final class will be the mode of that leaf node. In this study we use two parameters to control the pruning process including confidence or certainty factor (CF) and the smallest number of samples that must be put in at least two of the splits ($S_{\min}$, also known as \textit{minCases}). C5.0 algorithm uses error-based pruning method (see \citet{Quinlan_1993_Book} for more details). The objective is to minimize the observed error rate. Therefore, error estimates for leaf nodes and subtrees are computed assuming that they were used to classify a set of unseen cases with a predicted error rate which is computed based on CF and binomial distribution of number of misclassified events (E) and all training data at that leaf (N). Decreasing CF and increasing $S_{\min}$ cause more heavy pruning process \citep[see][]{Kuhn_2017_Manual}.} In particular, we build a suite of trees for varying values of CF and $S_{\min}$.

% ORIGINAL:
% In the particular case of the C5.0 algorithm the result of this process depends on several parameters, logical and numerical. We set the options to do feature selection and evaluate possible advanced splits of data, but do not use the boosted models option \citep[see][]{Kuhn_2017_Manual}; and use the confidence factor (CF) and the smallest number of samples that must be placed in at least two of the splits ($S_{\min}$, also known as \textit{minCases}) to fine tune the prediction models. In particular, we build a suite of trees for varying values of CF and $S_{\min}$.

Each tree resulting from this process has different qualities. We are interested in selecting a tree that is highly effective, but with a low level of complexity. In other words, we seek a tree with a good ratio of accuracy for predicting the classification outcome of the data, while using a reduced number of attributes (GOF metrics) in only a few number of steps (as given by a small number of decision nodes, or a tree with shallow depth). The number of metrics, the number of nodes, and the depth of a tree are directly seen from the topology of the tree, and are often proportional (shallow trees tend to use less nodes, and thus less metrics). In general, smaller trees are preferable because they are easy to understand and often more accurate predictors \citep{Quinlan_1996_JAIR}.

The effectiveness of the tree, on the other hand, needs to be measured. To that end we use the factor $F_{\beta}$ proposed by \citet{Rijsbergen_1979_Book} as
%
\begin{equation}
	\label{eq:f}
	F_{\beta} = \frac{ (1 + \beta^2) P R}{\beta^2 P + R}
	\, ,
\end{equation}
%
where $P$ and $R$ are the precision and recall factors, respectively, and $\beta$ is a weighting factor between the two. $P$ and $R$ are defined as
% $P$ measures the fraction of data samples that are correctly classified \change{in that class, whereas $R$ measures the fraction that are correctly recalled.}
%
% \textcolor{red}{of the data classified within that category regardless of whether it was correctly classified or not.}
%
% Mathematically, they are defined as
%
\begin{equation}
	P = \frac{ \mathrm{TP} }{ \mathrm{TP} + \mathrm{FP} }
	\quad \mathrm{and} \quad
	R = \frac{ \mathrm{TP} }{ \mathrm{TP} + \mathrm{FN} }
	\, .
\end{equation}
%
Here, TP, FP, and FN are the number of samples in the testing dataset considered to be true-positives, false-positives, and false-negatives, respectively.

These values are typically ordered in a confusion matrix. For the simplest case of two categories, a confusion matrix takes the form
%
\begin{equation}
\nonumber
\begin{array}{rr@{}rl}
	&					&	\multicolumn{2}{c}{\mathrm{Prediction}}		\\
	&					&	\mathrm{Positive}	&	\mathrm{Negative}	\\[0.75ex]
	\mathrm{Actual~Value}
	&	\begin{array}{r}
			\mathrm{Positive} \\[1ex]
			\mathrm{Negative}
		\end{array}
	&	\left[~
		\begin{array}{c}
			\mathrm{TP} \\[1ex]
			\mathrm{FP}
		\end{array}\right.
	&
		\left.
		\begin{array}{cc}
			~\mathrm{FN} \\[1ex]
			~\mathrm{TN} \\[1ex]
		\end{array}~\right]
\end{array}
\end{equation}
%
where TN is the number of true-negative samples. \change{Confusion matrices can be larger depending on the number of classes. In our case, the confusion matrices are size 4$\times$4, in order to compare the actual number of data samples classified as P, F, G and E with the number of samples predicted by the decision trees for each one of the same validation classes.}

We compute $F_{\beta}$ in equation (\ref{eq:f}) using $\beta = 1$ to give equal weight to $P$ and $R$ \citep{McCarthy_1995_Proc}. This is done for all the trees obtained using all possible combinations of CF and $S_{\min}$ values. Then, we select trees with high levels of effectiveness as \change{measured} by $F_1$, commensurate to such trees having low complexities, i.e., reduced number of metrics and shallow depths, which is precisely the goal set at the start. Note that $F_1$ is obtained based on the testing dataset as opposed to the training dataset.

% The results as applied to our dataset are presented next.
