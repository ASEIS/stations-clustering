
\subsection{Decision Trees}
\label{sec:decision-tree}

Building decision trees is a supervised learning process used to approximate a target function as a sequence of disjunctive conditions designed to measure the effectiveness of a set of attributes to classify data and predict outcomes. The theory behind decision trees is well established \citep[e.g.,][]{Quinlan_1986_ML, Quinlan_1993_Book, Mitchell_1997_Book}. Decision trees are, however, non-unique. They depend on the dataset, the user parameters, and the algorithm employed to build them \citep{Murthy_1998_DMKD}. Therefore, before describing our choice of the decision tree algorithm and parameters, we revise three aspects about the dataset: data classes, attributes equivalence, and dataset balance.

First, by data classes we refer to whether the classes of data are properly distinguishable or not, and to whether the data attributes contribute to those classes or not. In our case, data classes are handled by means of the clustering process. As we described in the previous section, at the end of the clustering process, all the data samples in our dataset are labeled into one out the four validation categories. In decision trees, however, we refer to these categories as \textit{classes}. We are therefore interested in working with a labeled dataset or a properly \textit{classified} dataset using the validation classes P, F, G and E, which is something that has been guaranteed by the prior clustering process.

Second, by attributes equivalence, we refer to whether the data attributes are comparable with each other on equivalent terms or not. There are cases, either because of the method or because of the data, that the data attributes need to be standardized. In artificial neural networks, for instance, the data needs to be normalized on a $[-1,1]$ or $[0,1]$ scale. Normalization is also necessary when data attributes come in different unit scales or value ranges \citep[e.g.,][]{Wu_2010_JH}. In our case, the concept of \textit{attributes} in the context of the decision tree analysis is the same as the concept of \textit{features} in clustering or GOF metrics in validation. Since all our metrics are dimensionless and defined within the same numerical scale (varying from 0 to 10), normalization is not necessary and the process of building a decision tree is not susceptible to the attributes scales.

Last, there is the issue of balance. A balanced dataset is one that has about the same number of samples per class. Algorithms used for building decision trees tend to perform better with balanced datasets \citep[e.g.,][]{Branco_2016_ACMCS, Weiss_2003_JAIR}. Imbalanced datasets, on the other hand, are those with a significant disparity in the number of samples in each class. Imbalanced datasets can be improved by a resampling process. There are two basic resampling approaches: undersampling and oversampling \citep{Branco_2016_ACMCS}. Undersampling discards data from the subsets with larger number of samples. Oversampling replicates data until reaching appropriate number of samples. Both processes are done by randomly picking data to either be discarded or replicated. In our case, as we will see later, we apply oversampling to one of our dataset classes.

In general, oversampling increases the possibility of overfitting, which occurs when the training data leads the algorithm to produce a decision tree that predicts outcomes too close to or exactly the same as the training data. This is not desirable because such a tree would lead to inaccurate predictions for other unseen data. Overfitting can be avoided by applying heavy pruning methods. (Pruning, as the word implies, is the process of cutting branches off a tree. Heavy pruning methods limit the complexity of the tree by constraining the number of branches and/or depth of a tree.)

Upon completing the clustering and resampling processes, the next step is to subdivide the dataset in two, a training dataset (with 70 percent of the samples, picked randomly), and a testing dataset (with the remaining 30 percent). We first use the training dataset to build a large suite of potential decision trees, and then use the testing dataset to evaluate and pick the best possible decision tree(s). There are several algorithms for building decision trees \citep[e.g., ID3, C4.5, C5.0, CART; see][]{Quinlan_1986_ML, Quinlan_1993_Book, Quinlan_1996_JAIR, Breiman_1984_Book}. Here, we use the C5.0 algorithm as implemented by \citet{Kuhn_2017_Manual}. This algorithm is the latest update to the original ID3 and subsequent C4.5 algorithms \citep{Quinlan_1993_Book, Quinlan_1996_JAIR}. C5.0 is superior to its predecessors because it reduces the limitations for handling numerical attributes, missing data, and it has additional features such as the development of boosted models. Discussing the differences between these algorithms is out of the scope of the paper.

In general, given a dataset, the process of building the tree consists of recursively identifying the attributes in the training data that are most likely to predict an outcome. The process is recursive because new branches and decision nodes are created based on the remaining data at every branch and level, until the tree reaches a certain depth or when other conditions are met. The effectiveness of an attribute $A$ in classifying any subset $S$ from the training data is measured through the information gain $G$ as
%
\begin{equation}
	\label{eq:gain}
	G(S,A) = E(S) - \sum_{v \, \in \, A[a]} \frac{ \left| S_v \right| }{ \left| S \right| } E \left( S_v \right)
	\, ,
\end{equation}
%
where $A[a]$ is the discrete set of all possible values of attribute $A$, $S_v$ is the subset of $S$ for which attribute $A$ has value $v$, and $E(\cdot)$ is the entropy function given by
%
\begin{equation}
	E(S) = \sum_{i=1}^{c} \Big( -p_i \log_2 \left(p_i\right) \Big)
	\, ,
\end{equation}
%
where $p_i$ is the proportion of $S$ belonging to class $i$, and $c$ are the different values that a given target attribute can take.

Entropy measures the homogeneity of a set of data. The higher the entropy, the more even the distribution of the data across classes. Conversely, an extremely low value of entropy would mean most of the data falls within a particular class. With this in mind, the information gain $G$ of an attribute $A$ for the dataset $S$, or $G(S,A)$ in equation (\ref{eq:gain}), is the expected reduction in entropy caused by partitioning the dataset according to the attribute $A$ \citep{Mitchell_1997_Book}, which we consider to be discrete. For the selection of a threshold for a continuous attribute $A(a)$, please refer to \citet{Quinlan_1996_JAIR}.

In the particular case of the C5.0 algorithm, in addition to the general steps just described, the result of the process of building a decision tree also depends on other logical and numerical parameters. We set the options to: (a) perform feature selection or winnowing, (b) evaluate possible advanced splits of data, (c) use a confidence factor, and (d) use a threshold for the number of samples that go in the tree leaves. The feature selection option is used by the algorithm to choose the most important attribute over others; the option of evaluating advanced splits prevents the use of a hard threshold during the classification by considering different probabilities in assigning data to different classes; the confidence factor (CF) is used to control the level of pruning; and the threshold for the number of samples in a leaf is set to a minimum number $S_{\min}$ (also known as \textit{minCases}) to limit the level of complexity of the tree. Additional details about these options are available in \citet{Quinlan_1993_Book} and \citet{Kuhn_2017_Manual}. For this study, in order to obtain a variety of tree options, we build trees for varying values of CF and $S_{\min}$.

Each tree resulting from this process has different qualities. We are interested in selecting a tree that is highly effective, but with a low level of complexity. In other words, we seek a tree with a good ratio of accuracy for predicting the classification outcome of the data, while using a reduced number of attributes (GOF metrics) in only a few number of steps (as given by a small number of decision nodes, or a tree with shallow depth). The number of metrics, the number of nodes, and the depth of a tree are directly seen from the topology of the tree, and are often proportional (shallow trees tend to use less nodes, and thus less metrics). In general, smaller trees are preferable because they are easy to understand and often more accurate predictors \citep{Quinlan_1996_JAIR}.

The effectiveness of the tree, on the other hand, needs to be measured. To that end we use the factor $F_{\beta}$ proposed by \citet{Rijsbergen_1979_Book} as
%
\begin{equation}
	\label{eq:f}
	F_{\beta} = \frac{ (1 + \beta^2) P R}{\beta^2 P + R}
	\, ,
\end{equation}
%
where $P$ and $R$ are the precision and recall factors, respectively, and $\beta$ is a weighting factor between the two. $P$ and $R$ are defined as
%
\begin{equation}
	P = \frac{ \mathrm{TP} }{ \mathrm{TP} + \mathrm{FP} }
	\quad \mathrm{and} \quad
	R = \frac{ \mathrm{TP} }{ \mathrm{TP} + \mathrm{FN} }
	\, ,
\end{equation}
%
respectively. Here, TP, FP, and FN are the number of samples in the testing dataset considered to be true-positives, false-positives, and false-negatives, respectively.

These values are typically ordered in a confusion matrix. For the simplest case of two categories, a confusion matrix takes the form
%
\begin{equation}
\nonumber
\begin{array}{rr@{}rl}
	&					&	\multicolumn{2}{c}{\mathrm{Prediction}}		\\
	&					&	\mathrm{Positive}	&	\mathrm{Negative}	\\[0.75ex]
	\mathrm{Actual~Value}
	&	\begin{array}{r}
			\mathrm{Positive} \\[1ex]
			\mathrm{Negative}
		\end{array}
	&	\left[~
		\begin{array}{c}
			\mathrm{TP} \\[1ex]
			\mathrm{FP}
		\end{array}\right.
	&
		\left.
		\begin{array}{cc}
			~\mathrm{FN} \\[1ex]
			~\mathrm{TN} \\[1ex]
		\end{array}~\right]
\end{array}
\end{equation}
%
where TN is the number of true-negative samples. Confusion matrices can be larger depending on the number of classes. In our case, the confusion matrices are size 4$\times$4, in order to compare the actual number of data samples classified as P, F, G and E with the number of samples predicted by the decision trees for each one of the same validation classes.

We compute $F_{\beta}$ in equation (\ref{eq:f}) using $\beta = 1$ to give equal weight to $P$ and $R$ \citep{McCarthy_1995_Proc}. This is done for all the trees obtained using all possible combinations of CF and $S_{\min}$ values. Then, we select trees with high levels of effectiveness as measured by $F_1$, commensurate to such trees having low complexities, i.e., reduced number of metrics and shallow depths, which is precisely the goal set at the start. Note that $F_1$ is obtained based on the testing dataset as opposed to the training dataset.
