
\introduction

Verification and validation of ground motion synthetics have received increasing attention in recent years due to advances in deterministic and non-deterministic physics-based earthquake ground motion simulation. Various methods or schemes have been proposed to evaluate, through direct quantitative comparisons or indirect statistical analysis, the similarity between simulation synthetics and recorded data, or with respect to other reference solutions \citep{Anderson_2004_Proc, Kristekova_2006_BSSA, Kristekova_2009_GJI, Olsen_2010_SRL, Burks_BSSA_2014, Rezaeian_2015_BSSA}. Among these methods, there are some that are better suited for verification purposes because they compare the signals at the waveforms level \citep{Kristekova_2006_BSSA, Kristekova_2009_GJI}, whereas others are better suited for validation. Among the latter, some are better for direct application purposes because they compare the behavior of systems (e.g., buildings or earth structures) at the response level \citep{Burks_BSSA_2014}, while the remaining methods focus on the validation of the overall characteristics of seismograms. In this category, the comparisons are done both in time and frequency. This type of validation is better suited for high-frequency or broadband simulations, where applications may be undefined and for which the expectation of matching seismograms at the waveform level is less relevant (unless dealing with low-frequencies, \fmaxleq{1}). Within this group, methods such as those proposed by \citet{Anderson_2004_Proc} and \citet{Olsen_2010_SRL} are preferred because they offer simple schemes to quantify the goodness-of-fit (GOF) of synthetics with respect to data in a broad sense.

Out of the various validation methods just described, \citet{Anderson_2004_Proc} is perhaps the most widely used today \citep[e.g.,][]{Chaljub_2010_BSSA, Bielak_2010_GJI, Guidotti_2011_SRL, Maufroy_2015_BSSA}. We, in particular, have consistently relied on a modified version of this method for the validation of a series of simulations of the 2008 $M_w$ 5.4 Chino Hills earthquake, and for the evaluation of velocity models in southern California through the validation of a large suite of moderate-magnitude earthquake simulations \citep{Taborda_2013_BSSA, Taborda_2014_BSSA, Taborda_2016_GJI}. 

In essence, \citeauthor{Anderson_2004_Proc}'s method assesses the similarity of two signals based on the average score of 10 metrics. These metrics measure the cumulative strength of the signals (in terms of Arias intensity and absolute energy), the comparative evolution of the signals in time (through normalized Arias and energy integrals), their peak values (in acceleration, velocity and displacement), frequency content (in terms of Fourier and response spectra), and their time synchronization (through cross-correlation). 

The strength of the method proposed by \citet{Anderson_2004_Proc} is that its metrics are well understood in both engineering and seismology contexts. However, it has been pointed out that some of these metrics are redundant, or that other relevant metrics need to be added. \citet{Taborda_2013_BSSA}, for instance, included---explicitly---the strong motion duration \citep{Trifunac_1975_BSSA} as an additional metric, and averaged the Arias intensity- and energy integral-related scores to avoid duplication. In the same spirit, \citet{Maufroy_2015_BSSA} reduced the number of metrics, limiting them to only those with comparable units. Unfortunately, none of these alternatives addresses the underlying questions: what are the parameters that ought to be taken into account when validating ground motion simulations, and what level of priority should these be given.

Lack of consensus about how to answer these questions makes the choice of validation methods and the selection of the comparison metrics a subjective one, mostly based on personal preferences and---arguably---on the application intended for the simulation and/or the validation itself. At the same time, with the current multiplicity of metrics, to give to all metrics the same weight in a validation analysis, makes it difficult for simulators to identify the sort of changes needed in their models that could lead to better ground motion predictions. We think that this situation can be corrected if we can offer data-informed arguments that can help simulators justify focusing on a reduced number of alternative metrics.

To that end, we study the relationships that exist between eleven different metrics---those proposed by \citet{Anderson_2004_Proc}, plus the strong motion duration---with the objective of offering a prioritized reduced number of metrics that can help predict validation results. We treat these metrics as independent variables defining a multi-dimensional space, and analyze them using machine learning techniques. We use a constrained \kmeans{} method \citep[e.g.,][]{Macqueen_1967_Proc, Wagstaff_2001_Proc} and conduct a subspace clustering analysis to address high-dimensional effects in order to add labels to the dataset in terms of four categories: poor, fair, good, and excellent. These categories adhere to the common practice in validation. The dataset used corresponds to a series of simulations for the 2008 \eqmag{w} 5.4 Chino Hills, California, earthquake, and their corresponding validation results \citep{Taborda_2014_BSSA}. Upon labeling the dataset, we develop two decision trees using a C5.0 algorithm \citep[][]{Quinlan_1993_Book, Quinlan_1996_JAIR} to prioritize the metrics involved in the validation process. The decision trees narrow the number of metrics offering a prediction algorithm into the aforementioned validation categories.

In summary, our results indicate that among the eleven metrics considered in the analysis, the acceleration response spectra and total energy of velocity are the most dominant ones, followed by the peak ground response in terms of velocity and acceleration. We test our prediction model and offer a discussion about its implications and potential use in future validation efforts.

% ===========================================================================================
%
% OLD NAEEM VERSION
% 
% The validation of ground motion synthetics has received increased attention over the last few years due to the advances in physics based deterministic and hybrid simulation methods. Unless in very low frequencies (\fleq{0.5}), due to limitations and uncertainties in the ground motion simulation and velocity models, it is not possible to match the simulation and synthetics wiggle by wiggle. Therefor, in order to compare synthetic seismogram with recorded data as a complex time series, different metrics are defined. These metrics are means to characterize the how well the synthetic matches the statistical characteristic of observed data. In general, these metrics compute the goodness-of-fit (GOF) between data and synthetic in time and frequency domains. \citet{Anderson_2004_Proc} proposed 10 metrics to be used to characterize the GOF in different frequency bands. \citet{Kristekova_2006_BSSA,Kristekova_2009_GJI} developed envelope and phase misfit criteria as well as GOF metrics for realistic scenarios. \citet{Olsen_2010_SRL} presented an alternative GOF metrics for broadband synthetics with relatively high resolution of the small misfits, and \citet{Taborda_2013_BSSA} added modification to \citet{Anderson_2004_Proc} metrics.

% These metrics have been widely used in different studies. Recently, due to advances in computational resources, 3D ground motion simulation on a regional scale were conducted in different regions using different methods \citep[e.g.,][]{Taborda_2016_GJI,Komatitsch_2004_BSSA,Bielak_2010_GJI,Roten_2016_GJI}. The accuracy of simulation with observed data are measured using these GOF scores. However, researchers use these metrics in different perspectives which is highly dependent on their application. For example, for structural engineers who are interested in designing tall buildings, among other metrics, displacement could be an important factor. However, for rigid and semi-rigid structures like dams and nuclear power plants, acceleration could be more important criteria. In general, for all purposes, the duration of strong ground morion could be an important factor. Analysis of the strong ground motion simulation for a specific application based on individual metrics is an acceptable practice, however, validating the simulation process only with individual score is not a valid approach. Previous studies show that metrics could have completely different scores for a pair of synthetic and data \citep[e.g.,][]{Taborda_2016_GJI}. \citet{Taborda_2013_BSSA} defined a metric from a combination of  \citet{Anderson_2004_Proc} scores presenting a general overview of GOF. However, the metric is a linear combination and somehow is biased by the authors' preferences about the metrics.

% Physics-based ground motion simulation communities push the maximum frequency  of the simulation to higher values (\fmax{}$\geq 5 ~ Hz$) and there is a high demand for a metric to uniformly validate the simulations. Although \citet{Anderson_2004_Proc}, defined different level for classifying the stations as poor, fair, good, and excellent based on individual metrics, there is no consensus on the most important metrics, specially when they have significantly different GOF score. Therefore, the question "How one can estimate the accuracy of simulation based on all scores or which metrics can be good representatives of simulation accuracy on behalf of other metrics?" has not addressed. This research question motivated us to approach the problem from different perspective. In other words, if we assume we know the accuracy of the simulation process (let's say we classified it as good) what is the relationship between different GOF scores for that pair of stations or which metrics are the best representer in classifying the stations. However, as far as the authors knowledge, there is no metric to be able to classify a pair of stations based on GOF scores as an estimation of the overall accuracy of the simulation. Therefore, as a first step, if we label the pair of station for simulation accuracy we can discuss the relationships between GOF scores. To do this we need a dataset where involves GOF scores for a good amount of stations.

% \citet{Taborda_2014_BSSA} conducted a comprehensive study to analyze the functionality of different velocity models in the Los Angeles basin. They used 2008 Chino Hills earthquakes based on 3 velocity models and conducted simulations with maximum frequency of \fmaxeq{4}. They compared the accuracy of the velocity models through using the GOF scores. The study generated an invaluable synthetic and data  GOF scores dataset.\\
% The first and the most common method in categorizing multidimensional unlabeled data is clustering and specifically the distance based methods like \kmeans{} \citep{Macqueen_1967_Proc}. However, there is a drawback in this method. If the data is not explicitly distinguishable the final clusters are highly dependent on initial starting points of the algorithm. Therefor, for stations with 11 dimensions, the ordinary \kmeans{} approach results in fairly different clusters after each analysis. The main reason for this issue is the algorithm solely looks for data which are close together in terms of Euclidean distance. However, in our case we have one more extra information. Hypothetically, if we have a pair of synthetic and data with GOF score equal 3 for all metrics, according to \citet{Anderson_2004_Proc} we can classify the station as poor with a strong confidence. This is also true for GOF score of 5, 7, and 9 for fair, good, and excellent classes, respectively. This background knowledge about the database could give a strong formation to the clustering process. \citet{Wagstaff_2001_Proc} defined a \kmeans{} clustering using background knowledge. Adding this feature to the algorithm improves the functionality and most importantly converge to the same clusters with any random initial clusters' centers. Using this approach alongside the subspace analysis (more details are presented in the section.~\ref{clustering}) we cluster the stations, then in a supervised learning algorithm, we develop a classification algorithm based on their GOF scores. We tune the learning algorithm and present the functionality of each prediction model based on different metrics. It is possible to have different classes for different components of a pair if synthetic and data. We study the different combination of components' classes in different distances and propose a rough estimate on deciding about the final GOF class of a pair of synthetic and data which is application and component independent.\\
% In the following section,we present a summary of the validation metrics then we discuss the dataset. Later on we explain the methodology. We provide a brief summary of unsupervised  and semi-supervised learning which are ordinary and constrained clustering approaches, respectively. We also discuss the basics of decision tree algorithm as a supervised learning method. We discuss the clustering challenges and use subspace analysis in the clustering analysis section. We also discuss the imbalanced data effects and solutions in the classification model section. We develop two decision tree classifiers (prediction model) based on labeled data for future use in comparing data and synthetics. We propose an algorithm to give a rough estimate to have component independent prediction. We conclude the study by applying the generated classification methods on \citet{Taborda_2014_BSSA} dataset and present the spatial variation of the simulation goodness of fit classes on the Los Angles basin for three velocity models and three components in component dependent and for three velocity models in component independent approaches. 




