
\section{Introduction}

The validation of ground motion synthetics has received increased attention over the last few years due to the advances in physics based deterministic and hybrid simulation methods. Unless in very low frequencies (\fleq{0.5}), due to limitations and uncertainties in the ground motion simulation and velocity models, it is not possible to match the simulation and synthetics wiggle by wiggle. Therefor, in order to compare synthetic seismogram with recorded data as a complex time series, different metrics are defined. These metrics are means to characterize the how well the synthetic matches the statistical characteristic of observed data. In general, these metrics compute the goodness-of-fit (GOF) between data and synthetic in time and frequency domains. \citet{Anderson_2004_Proc} proposed 10 metrics to be used to characterize the GOF in different frequency bands. \citet{Kristekova_2006_BSSA,Kristekova_2009_GJI} developed envelope and phase misfit criteria as well as GOF metrics for realistic scenarios. \citet{Olsen_2010_SRL} presented an alternative GOF metrics for broadband synthetics with relatively high resolution of the small misfits, and \citet{Taborda_2013_BSSA} added modification to \citet{Anderson_2004_Proc} metrics.\\
These metrics have been widely used in different studies. Recently, due to advances in computational resources, 3D ground motion simulation on a regional scale were conducted in different regions using different methods \citep[e.g.,][]{Taborda_2016_GJI,Komatitsch_2004_BSSA,Bielak_2010_GJI,Roten_2016_GJI}. The accuracy of simulation with observed data are measured using these GOF scores. However, researchers use these metrics in different perspectives which is highly dependent on their application. For example, for structural engineers who are interested in designing tall buildings, among other metrics, displacement could be an important factor. However, for rigid and semi-rigid structures like dams and nuclear power plants, acceleration could be more important criteria. In general, for all purposes, the duration of strong ground morion could be an important factor. Analysis of the strong ground motion simulation for a specific application based on individual metrics is an acceptable practice, however, validating the simulation process only with individual score is not a valid approach. Previous studies show that metrics could have completely different scores for a pair of synthetic and data \citep[e.g.,][]{Taborda_2016_GJI}. \citet{Taborda_2013_BSSA} defined a metric from a combination of  \citet{Anderson_2004_Proc} scores presenting a general overview of GOF. However, the metric is a linear combination and somehow is biased by the authors' preferences about the metrics. \\
Physics-based ground motion simulation communities push the maximum frequency  of the simulation to higher values (\fmax{}$\geq 5 ~ Hz$) and there is a high demand for a metric to uniformly validate the simulations. Although \citet{Anderson_2004_Proc}, defined different level for classifying the stations as poor, fair, good, and excellent based on individual metrics, there is no consensus on the most important metrics, specially when they have significantly different GOF score. Therefore, the question "How one can estimate the accuracy of simulation based on all scores or which metrics can be good representatives of simulation accuracy on behalf of other metrics?" has not addressed. This research question motivated us to approach the problem from different perspective. In other words, if we assume we know the accuracy of the simulation process (let's say we classified it as good) what is the relationship between different GOF scores for that pair of stations or which metrics are the best representer in classifying the stations. However, as far as the authors knowledge, there is no metric to be able to classify a pair of stations based on GOF scores as an estimation of the overall accuracy of the simulation. Therefore, as a first step, if we label the pair of station for simulation accuracy we can discuss the relationships between GOF scores. To do this we need a dataset where involves GOF scores for a good amount of stations. \\  
 \citet{Taborda_2014_BSSA} conducted a comprehensive study to analyze the functionality of different velocity models in the Los Angeles basin. They used 2008 Chino Hills earthquakes based on 3 velocity models and conducted simulations with maximum frequency of \fmaxeq{4}. They compared the accuracy of the velocity models through using the GOF scores. The study generated an invaluable synthetic and data  GOF scores dataset.\\
The first and the most common method in categorizing multidimensional unlabeled data is clustering and specifically the distance based methods like \kmeans{} \citep{Macqueen_1967_Proc}. However, there is a drawback in this method. If the data is not explicitly distinguishable the final clusters are highly dependent on initial starting points of the algorithm. Therefor, for stations with 11 dimensions, the ordinary \kmeans{} approach results in fairly different clusters after each analysis. The main reason for this issue is the algorithm solely looks for data which are close together in terms of Euclidean distance. However, in our case we have one more extra information. Hypothetically, if we have a pair of synthetic and data with GOF score equal 3 for all metrics, according to \citet{Anderson_2004_Proc} we can classify the station as poor with a strong confidence. This is also true for GOF score of 5, 7, and 9 for fair, good, and excellent classes, respectively. This background knowledge about the database could give a strong formation to the clustering process. \citet{Wagstaff_2001_Proc} defined a \kmeans{} clustering using background knowledge. Adding this feature to the algorithm improves the functionality and most importantly converge to the same clusters with any random initial clusters' centers. Using this approach alongside the subspace analysis (more details are presented in the section.~\ref{clustering}) we cluster the stations, then in a supervised learning algorithm, we develop a classification algorithm based on their GOF scores. We tune the learning algorithm and present the functionality of each prediction model based on different metrics. It is possible to have different classes for different components of a pair if synthetic and data. We study the different combination of components' classes in different distances and propose a rough estimate on deciding about the final GOF class of a pair of synthetic and data which is application and component independent.\\
In the following section,we present a summary of the validation metrics then we discuss the dataset. Later on we explain the methodology. We provide a brief summary of unsupervised  and semi-supervised learning which are ordinary and constrained clustering approaches, respectively. We also discuss the basics of decision tree algorithm as a supervised learning method. We discuss the clustering challenges and use subspace analysis in the clustering analysis section. We also discuss the imbalanced data effects and solutions in the classification model section. We develop two decision tree classifiers (prediction model) based on labeled data for future use in comparing data and synthetics. We propose an algorithm to give a rough estimate to have component independent prediction. We conclude the study by applying the generated classification methods on \citet{Taborda_2014_BSSA} dataset and present the spatial variation of the simulation goodness of fit classes on the Los Angles basin for three velocity models and three components in component dependent and for three velocity models in component independent approaches. 




