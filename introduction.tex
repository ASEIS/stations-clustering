
\introduction

Verification and validation of ground motion synthetics have received increasing attention in recent years due to advances in deterministic and non-deterministic physics-based earthquake ground motion simulation, as well as a growing interest in the use of synthetic seismograms for engineering applications. Validation entails the comparison of simulations with observations, whereas verification involves the comparison of simulations with exact or alternative solutions \citep[see][and references therein]{Bielak_2010_GJI, Taborda_2013_BSSA}. Various methods or schemes have been proposed to evaluate, through direct signal-to-signal quantitative comparisons or overall statistical analyses, the similarity between simulation synthetics and recorded data, or with respect to other reference solutions \citep{Anderson_2004_Proc, Kristekova_2006_BSSA, Kristekova_2009_GJI, Olsen_2010_SRL, Burks_BSSA_2014, Rezaeian_2015_BSSA}. Some of these methods are better suited for verification purposes because they compare the signals at the waveforms level \citep{Kristekova_2006_BSSA, Kristekova_2009_GJI}, whereas others are better suited for validation. \citet{Burks_BSSA_2014} and \citet{Rezaeian_2015_BSSA}, for instance, are used for validation in the context of engineering applications, i.e., at the level of the dynamic response of buildings. \citet{Anderson_2004_Proc} and \citet{Olsen_2010_SRL}, on the other hand, are used for validation in the context of the overall characteristics of seismograms when comparing synthetics with data, in both time and frequency. These methods are preferred for validating high-frequency or broadband simulations, where applications may be undefined or the expectation of matching seismograms at the waveforms level is less relevant (unless dealing with low-frequencies, \fmaxleq{1}), and because they offer simple schemes to quantify the goodness-of-fit (GOF) of the simulation results.

Out of the various validation methods just described, \citet{Anderson_2004_Proc} is perhaps the most widely used today \citep[e.g.,][]{Chaljub_2010_BSSA, Bielak_2010_GJI, Guidotti_2011_SRL, Maufroy_2015_BSSA}. In essence, this method assesses the similarity of two signals based on the average score of 10 metrics. These metrics measure the cumulative strength of the signals (in terms of Arias intensity and absolute energy), the comparative evolution of the signals in time (through normalized Arias and energy integrals), their peak values (in acceleration, velocity and displacement), frequency content (in terms of Fourier and response spectra), and their time synchronization (through cross-correlation). We, in particular, have consistently relied on a modified version of \citeauthor{Anderson_2004_Proc}'s method for the validation of a series of simulations of the 2008 $M_w$ 5.4 Chino Hills earthquake \citep{Taborda_2013_BSSA, Taborda_2014_BSSA}, and for the evaluation of velocity models in southern California through the validation of a large suite of moderate-magnitude earthquake simulations \citep{Taborda_2016_GJI}.

The strength of the method proposed by \citet{Anderson_2004_Proc} is that its metrics convey physical meaning to both seismologists and engineers. However, it has been pointed out that some of these metrics are redundant, or that other relevant metrics need to be added. \citet{Taborda_2013_BSSA}, for instance, included---explicitly---the strong motion duration \citep{Trifunac_1975_BSSA} as an additional metric, and averaged the Arias intensity- and energy integral-related scores to avoid duplication. In the same spirit, \citet{Maufroy_2015_BSSA} reduced the number of metrics, limiting them to only those with comparable units. Unfortunately, none of these alternatives addresses the underlying questions regarding what are the most important parameters that ought to be taken into account when validating ground motion simulations for a specific application, and what level of priority should these parameters be given.

Lack of consensus about how to answer these questions makes the choice of validation methods and the selection of the comparison metrics a subjective one, mostly based on personal preferences and---arguably---on the application intended for the simulation and/or the validation itself. At the same time, with the current multiplicity of metrics, to give to all metrics the same weight in a validation analysis, makes it difficult for simulators to identify the sort of changes needed in their models that could lead to better ground motion predictions. We think that this situation can be corrected if we can offer data-informed arguments that can help simulators justify focusing on a reduced number of alternative metrics.

To that end, we study the relationships that exist between eleven different metrics---those proposed by \citet{Anderson_2004_Proc}, plus the strong motion duration---with the objective of offering a prioritized reduced number of metrics that can help predict validation results. We initially treat these metrics as independent variables defining a multi-dimensional space, and analyze them using machine learning techniques. In particular, we use semi-supervised and supervised methods. In machine learning, whether a method is considered unsupervised, semi-supervised or supervised, depends on the level of constraints applied to the process or prior level of classification given to the data. Here, we first use a semi-supervised method called \textit{constrained k-means} in order to conduct a subspace clustering analysis to label data samples in a given validation dataset \citep[e.g.,][]{Macqueen_1967_Proc, Wagstaff_2001_Proc}. This dataset corresponds to the validation results from a series of simulations done for the 2008 \eqmag{w} 5.4 Chino Hills, California, earthquake \citep{Taborda_2014_BSSA}. Here, the clustering done by the constrained $k$-means method allows us to label the data using for validation categories: poor, fair, good, and excellent. These categories were defined by \citet{Anderson_2004_Proc} and have been adopted---despite some differences---by other GOF methods used in verification and validation \citep[e.g.,][]{Kristekova_2009_GJI, Olsen_2010_SRL}. Upon labeling the data samples in our dataset into these four categories, we use a supervised learning method as implemented in the C5.0 algorithm to obtain a family of decision trees \citep[][]{Quinlan_1993_Book, Quinlan_1996_JAIR}. We then select a few of these trees to gain insight about and prioritize the metrics involved in the validation process. The decision trees narrow the number of metrics offering a prediction algorithm into the aforementioned validation categories.

In summary, although specific to the chosen dataset (from the given set of simulations for a single event), our results indicate that among the eleven metrics considered in the analysis, the acceleration response spectra and total energy of velocity are the most dominant ones, followed by the peak ground response in terms of acceleration and velocity. We test our prediction model and offer a discussion about its implications and potential use in future validation efforts.
