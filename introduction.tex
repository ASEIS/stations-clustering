
\introduction

Verification and validation of ground motion synthetics have received increasing attention in recent years due to advances in deterministic and non-deterministic physics-based earthquake ground motion simulation. Various methods or schemes have been proposed to evaluate, through direct quantitative comparisons or indirect statistical analysis, the similarity between simulation synthetics and recorded data, or with respect to other reference solutions \citep{Anderson_2004_Proc, Kristekova_2006_BSSA, Kristekova_2009_GJI, Olsen_2010_SRL, Burks_BSSA_2014, Rezaeian_2015_BSSA}. Among these methods, there are some that are better suited for verification purposes because they compare the signals at the waveforms level \citep{Kristekova_2006_BSSA, Kristekova_2009_GJI}, whereas others are better suited for validation. Among the latter, some are better for direct application purposes because they compare the behavior of systems (e.g., buildings or earth structures) at the response level \citep{Burks_BSSA_2014}, while the remaining methods focus on the validation of the overall characteristics of seismograms. In this category, the comparisons are done both in time and frequency. This type of validation is better suited for high-frequency or broadband simulations, where applications may be undefined and for which the expectation of matching seismograms at the waveform level is less relevant (unless dealing with low-frequencies, \fmaxleq{1}). Within this group, methods such as those proposed by \citet{Anderson_2004_Proc} and \citet{Olsen_2010_SRL} are preferred because they offer simple schemes to quantify the goodness-of-fit (GOF) of synthetics with respect to data in a broad sense.

Out of the various validation methods just described, \citet{Anderson_2004_Proc} is perhaps the most widely used today \citep[e.g.,][]{Chaljub_2010_BSSA, Bielak_2010_GJI, Guidotti_2011_SRL, Maufroy_2015_BSSA}. We, in particular, have consistently relied on a modified version of this method for the validation of a series of simulations of the 2008 $M_w$ 5.4 Chino Hills earthquake, and for the evaluation of velocity models in southern California through the validation of a large suite of moderate-magnitude earthquake simulations \citep{Taborda_2013_BSSA, Taborda_2014_BSSA, Taborda_2016_GJI}. 

In essence, \citeauthor{Anderson_2004_Proc}'s method assesses the similarity of two signals based on the average score of 10 metrics. These metrics measure the cumulative strength of the signals (in terms of Arias intensity and absolute energy), the comparative evolution of the signals in time (through normalized Arias and energy integrals), their peak values (in acceleration, velocity and displacement), frequency content (in terms of Fourier and response spectra), and their time synchronization (through cross-correlation). 

The strength of the method proposed by \citet{Anderson_2004_Proc} is that its metrics are well understood in both engineering and seismology contexts. However, it has been pointed out that some of these metrics are redundant, or that other relevant metrics need to be added. \citet{Taborda_2013_BSSA}, for instance, included---explicitly---the strong motion duration \citep{Trifunac_1975_BSSA} as an additional metric, and averaged the Arias intensity- and energy integral-related scores to avoid duplication. In the same spirit, \citet{Maufroy_2015_BSSA} reduced the number of metrics, limiting them to only those with comparable units. Unfortunately, none of these alternatives addresses the underlying questions: what are the parameters that ought to be taken into account when validating ground motion simulations, and what level of priority should these be given.

Lack of consensus about how to answer these questions makes the choice of validation methods and the selection of the comparison metrics a subjective one, mostly based on personal preferences and---arguably---on the application intended for the simulation and/or the validation itself. At the same time, with the current multiplicity of metrics, to give to all metrics the same weight in a validation analysis, makes it difficult for simulators to identify the sort of changes needed in their models that could lead to better ground motion predictions. We think that this situation can be corrected if we can offer data-informed arguments that can help simulators justify focusing on a reduced number of alternative metrics.

To that end, we study the relationships that exist between eleven different metrics---those proposed by \citet{Anderson_2004_Proc}, plus the strong motion duration---with the objective of offering a prioritized reduced number of metrics that can help predict validation results. We treat these metrics as independent variables defining a multi-dimensional space, and analyze them using machine learning techniques. We use a constrained \kmeans{} method \citep[e.g.,][]{Macqueen_1967_Proc, Wagstaff_2001_Proc} and conduct a subspace clustering analysis to address high-dimensional effects in order to add labels to the dataset in terms of four categories: poor, fair, good, and excellent. These categories adhere to the common practice in validation. The dataset used corresponds to a series of simulations for the 2008 \eqmag{w} 5.4 Chino Hills, California, earthquake, and their corresponding validation results \citep{Taborda_2014_BSSA}. Upon labeling the dataset, we develop two decision trees using a C5.0 algorithm \citep[][]{Quinlan_1993_Book, Quinlan_1996_JAIR} to prioritize the metrics involved in the validation process. The decision trees narrow the number of metrics offering a prediction algorithm into the aforementioned validation categories.

In summary, our results indicate that among the eleven metrics considered in the analysis, the acceleration response spectra and total energy of velocity are the most dominant ones, followed by the peak ground response in terms of velocity and acceleration. We test our prediction model and offer a discussion about its implications and potential use in future validation efforts.
