
\section{Data Analysis Method} 
\label{sec:approach}

We are interested in developing a decision-making algorithm with a reduced and prioritized number of validation metrics based on previously acquired validation data (i.e., our dataset). A common method to do this is to identify rules with disjunctive characteristics in the form of a decision tree. The end-goal is to have a decision tree that leads to an outcome that is representative of the overall quality of the comparisons between two signals. In our case, we define such outcome in terms of four attributes representative of the quality of the validation, namely: poor, fair, good, and excellent.

In machine learning, decision trees are classified as a supervised learning method, and the first step towards designing them requires that the data be labeled according to their attributes. The inherent attribute of our data are the GOF value themselves, but because of the multiplicity of metrics and the lack of clarity about how these relate to each other, we need to add labels to the data that are in accordance with the outcome attributes, i.e., in terms of the predefined validation quality levels. We label the data in our dataset by means of a clustering process. 

There exist different methods for clustering data \citep[see Chapters 10 and 11 in][]{Han_2011_Book}, among which the \kmeans{} and constrained \kmeans{} clustering methods are the most widely used \citep{Jain_1999_ACMCS}. We note about these methods that the \kmeans{} approach is sensitive to the initial values chosen to be at the center of the clusters---especially in the case of data that are not clearly distinguishable---, whereas the constrained \kmeans{} method uses background knowledge to overcome this limitation. Because of its use of background information, the modified \kmeans{} method is considered as a semi-supervised process.

Once the data has been properly labeled, one can use a supervised approach to generate the decision tree through a recursive search for the best hypotheses to classify the outcome of the simulation based on a given sub-set of metrics. The following two sections explain the data processing analysis we have put in place to cluster our dataset and obtain the decision tree(s).

\input{clustering}

\input{decision-tree}

% =========================================================================================================================
% 
% NAEEM NEW SUGGESTION
% 
% Prioritized, reduced number of validation metrics requires developing a decision-making algorithm. A common method to do this is to identify rules with disjunctive characteristics in the form of a decision tree. Decision tree is considered as a supervised learning method. Therefore, we need to know the overall GOF of each observation in the dataset. Or in other words if we consider the simulation process for that specific station and component as poor, fair, good, or excellent process.  Therefore, the first step in such an approach requires that the data be labeled according to their attributes. The inherent attribute of our data is the GOF value, but need to label the data in terms of the outcome attribute. Generating labels for data is mainly possible through a clustering process. There exist different methods for clustering data (see Chapter 10 and 11 of Han 2011). The k-means and constrained k -means clustering methods are among the most widely used Ease of implementation, simplicity, efficiency, and empirical success are the main reasons for its popularity (Jain 2010). We note about these methods that the k –means approach is sensitive to the initial values chosen as the center of the clusters—especially in the case of data that are not clearly distinguishable—, whereas the constrained k –means method uses background knowledge to overcome this limitation. Because of its used of background information, the modified k -means method is considered as a semi-supervised process. Once the data has been properly labeled, one can use a supervised approach to generate the decision tree that can serve as a prediction model for the outcome. In essence, the process of designing the decision tree is a search for the best hypothesis to classify the outcome of the simulation, i.e., the similarity between synthetic and recorded seismograms, based on a given sub-set of metrics. The following two sections explain the data processing analysis we have put in place to achieve this.

% =========================================================================================================================
% 
% RICARDO FIRST PASS

% In order to study the relationships that exist between the different validation metrics and offer a prioritized, reduced number of metrics that can help predict validation results, we need to identify and classify patterns in the dataset. A common method to do this is to identify rules with disjunctive characteristics in the form of a decision tree. Among the available approaches used to design such a tree there are machine learning methods that use supervised and unsupervised learning. The first step in such an approach requires that the data be labeled according to their attributes. The inherent attribute of our data is the GOF value, but need to label the data in terms of the outcome attribute. Following common practice, in our case, we want to label the dataset values as poor, fair, good, or excellent.

% Generating labels for data in an automated way is possible through a clustering process. There exist different methods for clustering data (see \textcolor{red}{a reference about clustering}). The \kmeans{} and modified \kmeans{} clustering methods are among the most widely used (\textcolor{red}{a reference to back up this statement}). We note about these methods that the \kmeans{} approach is sensitive to the initial values chosen at the center of the clusters---especially in the case of data that are not clearly distinguishable---, whereas the modified \kmeans{} method uses background knowledge to overcome this limitation. Because of its used of background information, the modified \kmeans{} method is considered as a semi-supervised process.

% Once the data has been properly labeled, one can use a supervised approach to generate the decision tree that can serve as a prediction model for the outcome. In essence, the process of designing the decision tree is a search for the best hypothesis to classify the outcome of the simulation, i.e., the similarity between synthetic and recorded seismograms, based on a given sub-set of metrics. The following two sections explain the data processing analysis we have put in place to achieve this.

% \input{clustering}

% ===========================================================================================
%
% OLD NAEEM VERSION
% 
% As we discus in section.\ref{validation_metrics}, there is no consensus on a single metric who able to estimate the accuracy of the simulation or can classify the simulation based on given GOF scores. On the other hand we have a comprehensive dataset. Therefore we need to conduct a data mining and knowledge discovery process to get the classification pattern out of the data set. A common method for generating a decision rule having disjunctive characteristic is decision tree approach. The method is developing a rule for classifying data (here stations) based on different attributes (here GOF scores), this process also called supervised learning in machine learning community. However, decision tree needs labeled data. In our case labeled data would be a data with another attribute which shows the overall accuracy of the simulation (In other word the one metric that we are developing methods to generate it). Therefore we need to go one step back and generate label for the data. Generating labels for data is categorized as unsupervised learning or clustering. Different methods are developed for clustering. The most commonly used method is \kmeans{}, however, it gives different results based on initial values of clusters' centers especially in the case that data is not clearly distinguishable. To overcome this drawback, we use modified \kmeans{} approach. In this method one can use background knowledge in the unsupervised learning process. Using background knowledge in unsupervised learning converts it into a semi-supervised learning process.

% In summary, first we label the data based on the constrained \kmeans{} approach, then, in a classification process, using decision tree we search for the best hypothesis to classify a station based on GOF scores. In the rest of this section first we discuss the clustering method for both ordinary and constrained \kmeans{} approach then we discuss the decision tree algorithm and basics in classifying data. We discuss the application results in the clustering analysis and classification model sections. \\

% \input{clustering}
% \input{Dtree}

% More text for use
%========================= \\
 %The method uses background knowledge to direct the clustering in an appropriate direction so the results are close to real data. Obviously there will be some features that is not obey the general pattern among these features. We will put aside those features. Then we try to modify those features to be able to use again in the processing. In the other words we will scale those features to obey the general trend. Anderson(2004) divided the stations into four main groups based on those stations scores. In hypothetical case were two signals are identical these scores should be 10. However, if they are not the same, but the scoring method provides similar results we should get a fairly close score for all matrics. In real cases the scores are different from one another. However, if we could define a metrics that gives a similar level of goodness of fit for all of them then we can for sure categorized the station in an appropriate group. In this study we define 4 hypothetical stations with score of 3,5,7, and 9 which represent middle points for poor, fair, good and excellent goodness of fit categories. Any station close to these stations should reflect the idea that they are at the same group. However, they are slightly different and we want to know why and how we can make them similar. So having these hypothetical stations and forcing the constrained k-means algorithm to put these stations in different groups we are able to bring the background knowledge into the process. So in the study we start with one attribute and cluster stations. According to figures (2D and 3D) we can argue that the background knowledge assisted the k-means clustering to cluster the stations, realistically. Meanwhile it could not force the clusters to have a cluster centers as those points.  We assign the most frequent stations' cluster to that station. Then we study the score variation in each group and see if they have a uniform behavior if not we can further study the potential problems with that score and probable scaling factors to make it obey the pattern. We also analyze the effect of velocity model, earthquake, orientation, frequency band in the final results. 
%Based on having class for each station in the study we develop a decisition tree to provide the group of simulation for Los Angles Basin earthquakes.  \\

%To do list: \\

%Another approach to go is considering all data without separating for earthquake, velocity model, and so on. Then try to put them in one cluster. Those stations which are in the cluster but are in extreme case, remove from the data base. \\
%Analyze inside each cluster, provide normal distribution, if data looks ok keep it, if not remove those features. \\
%Normally majority of dimensions are irrelevant in high dimensional data. These irrelevant dimensions can hide clusters in noisy data and confuse the clustering algorithm. \\
%Try different method and cluster data, then find the points that clustered currently then use that points as a must link condition. \\
%We can not show data with 10 attributes in graph, however, we can generate a set of statistical figures of changing these values in the figure.\\ 
%If using a feature cause to not able to fit the station in any class we remove it or give a factor to put in the right class.\\
%in the end we want to have all stations as constrained. In other word, we start from 1D and if there is a station that is always in the same group we added it in constraint points. then we move to the 2D and we continue the same until we have all station as constraint.\\
%Constraint helps the clustering to get the same results after good amount of iterations.\\
%========================= \\