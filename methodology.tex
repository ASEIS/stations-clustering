
\section{Methodology} \label{methodology}

As we discus in section.\ref{validation_metrics}, there is no consensus on a single metric who able to estimate the accuracy of the simulation or could classify the simulation based on given GOF scores. On the other hand we have a comprehensive dataset. Therefore we need to conduct a data mining and knowledge discovery process to get the classification pattern out of the data set. A common method for generating a decision rule having disjunctive characteristic is decision tree approach. The method is developing a rule for classifying data (here stations) based on different attributes (here GOF scores), this process also called supervised learning in machine learning community. However, decision tree needs labeled data. In our case labeled data would be a data with another attribute which shows the overall accuracy of the simulation (In other word the one metric that we are developing methods to generate it). Therefore we need to go one step back and generate label for the data. Generating labels for data is categorized as unsupervised learning or clustering. Different methods have been developed for clustering. The most commonly used method is \kmeans{}. However, \kmeans{} method gives different results based on initial values of cluster center especially where data is not clearly distinguishable. In result we use modified \kmeans{} approach. This method use background knowledge in the unsupervised learning process. Using background knowledge in unsupervised learning converts it into semi-supervised learning process.\\
In summary, first we label the data based on the constrained \kmeans{} approach, then we statistically analyze the each group results. We expect to see similarity among attributes within each classes. Later on, in a classification process, using decision tree we find the best hypothesis to classify a station based on GOF scores. In the rest of this section first we discuss the clustering method for both ordinary and constrained \kmeans{} approach then we discuss the decision tree algorithm and basics in separating data. We discuss the statistical analysis of the results in the results section. \\


\subsection{Clustring}
\input{clustering}
\subsection{Decision Tree}
\input{Dtree}

% More text for use
%========================= \\
 %The method uses background knowledge to direct the clustering in an appropriate direction so the results are close to real data. Obviously there will be some features that is not obey the general pattern among these features. We will put aside those features. Then we try to modify those features to be able to use again in the processing. In the other words we will scale those features to obey the general trend. Anderson(2004) divided the stations into four main groups based on those stations scores. In hypothetical case were two signals are identical these scores should be 10. However, if they are not the same, but the scoring method provides similar results we should get a fairly close score for all matrics. In real cases the scores are different from one another. However, if we could define a metrics that gives a similar level of goodness of fit for all of them then we can for sure categorized the station in an appropriate group. In this study we define 4 hypothetical stations with score of 3,5,7, and 9 which represent middle points for poor, fair, good and excellent goodness of fit categories. Any station close to these stations should reflect the idea that they are at the same group. However, they are slightly different and we want to know why and how we can make them similar. So having these hypothetical stations and forcing the constrained k-means algorithm to put these stations in different groups we are able to bring the background knowledge into the process. So in the study we start with one attribute and cluster stations. According to figures (2D and 3D) we can argue that the background knowledge assisted the k-means clustering to cluster the stations, realistically. Meanwhile it could not force the clusters to have a cluster centers as those points.  We assign the most frequent stations' cluster to that station. Then we study the score variation in each group and see if they have a uniform behavior if not we can further study the potential problems with that score and probable scaling factors to make it obey the pattern. We also analyze the effect of velocity model, earthquake, orientation, frequency band in the final results. 
%Based on having class for each station in the study we develop a decisition tree to provide the group of simulation for Los Angles Basin earthquakes.  \\

%To do list: \\

%Another approach to go is considering all data without separating for earthquake, velocity model, and so on. Then try to put them in one cluster. Those stations which are in the cluster but are in extreme case, remove from the data base. \\
%Analyze inside each cluster, provide normal distribution, if data looks ok keep it, if not remove those features. \\
%Normally majority of dimensions are irrelevant in high dimensional data. These irrelevant dimensions can hide clusters in noisy data and confuse the clustering algorithm. \\
%Try different method and cluster data, then find the points that clustered currently then use that points as a must link condition. \\
%We can not show data with 10 attributes in graph, however, we can generate a set of statistical figures of changing these values in the figure.\\ 
%If using a feature cause to not able to fit the station in any class we remove it or give a factor to put in the right class.\\
%in the end we want to have all stations as constrained. In other word, we start from 1D and if there is a station that is always in the same group we added it in constraint points. then we move to the 2D and we continue the same until we have all station as constraint.\\
%Constraint helps the clustering to get the same results after good amount of iterations.\\
%========================= \\