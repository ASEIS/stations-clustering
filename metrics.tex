
\section{Validation Metrics} 
\label{sec:validation-metrics}

There are various methods and algorithms available to evaluate the similarity between two or more seismograms, both through direct \change{signal-to-signal} quantitative comparisons or \change{overall} statistical analyses \citep[e.g.,][]{Anderson_2004_Proc, Kristekova_2006_BSSA, Kristekova_2009_GJI, Olsen_2010_SRL, Burks_BSSA_2014, Rezaeian_2015_BSSA}. In earthquake ground motion simulation, they are used primarily for verification with respect to benchmark or analytical solutions, and for validation with respect to data (i.e., ground motion records). Here, we focus on the list of metrics proposed by \citet{Anderson_2004_Proc}, with an additional metric for duration, as suggested by others \citep{Olsen_2010_SRL, Maufroy_2015_BSSA} and as implemented in \citet{Taborda_2013_BSSA}. These metrics are listed in Table \ref{tab:metrics}.

\input{tab-metrics}

Following \citet{Anderson_2004_Proc}, when applied to a pair of signals, each one of these metrics yields a GOF score ranging from 0 to 10, where a value of 10 corresponds to two signals having identical characteristics. This scoring scale varies according to the following exponential function
% 
\begin{equation}
\label{eq:gof-function}
	S \left( p_1, p_2 \right) = 10 \exp{ \left[ - \left( \frac{p_1 - p_2}{ \min\left( p_1, p_2 \right) } \right)^2 \right] }
	\, ,
\end{equation}
% 
\noindent
where $S$ is the GOF score that results from comparing values $p_1$ and $p_2$ from signals 1 and 2, respectively, for each one of the different metrics in Table \ref{tab:metrics}. \change{In the case of C8 and C9, where the values $p_1$ and $p_2$ are function of the period ($T$) and frequency ($f$), respectively, $S$ is computed for all values of $T$ and $f$ and averaged to produce a single mean score.} \citeauthor{Anderson_2004_Proc} also provided guidelines for the process to be applied to the original signals as well as to those resulting from a sequential set of band-pass filters covering the whole frequency range of interest. In his method, this frequency-domain decomposition should have pass bands defined following a logarithmic distribution in order to give more weight to the low frequencies, and the GOF scores of all sub-bands and the broadband be averaged into a single GOF final score. \change{However, in this study, in order to avoid the additional parameters that would result from this approach \citep[i.e., sub-bands width and filter characteristics][]{Khoshnevis_2015_Proc}, we use only the broadband (\frange{0}{4}) results.}

One important aspect of \citet{Anderson_2004_Proc} is that the results of the GOF scores were calibrated by comparing horizontal components of recorded seismograms and other simulations using the first ten metrics (C1 through C10) in Table \ref{tab:metrics} in order to find out how the scores were distributed throughout the scoring scale. \citeauthor{Anderson_2004_Proc} concluded that for a typical distribution of scores, the GOF values could be classified into four validation categories: poor (for scores from 0 to 4), fair (4 to 6), good (6 to 8), and excellent (for scores from 8 to 10). While these categories are arguably subjective, over the years they have \change{been adopted---despite some differences in the ranges---by other GOF methods employed in verification and validation \citep[e.g.,][]{Kristekova_2009_GJI, Olsen_2010_SRL}. As a result, \citeauthor{Anderson_2004_Proc}'s method has been used as} a reference baseline for various validation studies \citep[e.g.,][]{Chaljub_2010_BSSA, Bielak_2010_GJI, Guidotti_2011_SRL, Maufroy_2015_BSSA}.

Here, we focus our analysis on the eleven metrics included in Table \ref{tab:metrics}, and investigate the relationships that exist between them in order to prioritize a reduced number of metrics that can help predict the outcome category one would use to label the results of a given simulation.
